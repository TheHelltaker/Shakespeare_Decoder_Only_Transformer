{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "56990155",
      "metadata": {
        "id": "56990155"
      },
      "source": [
        "## Decoder only Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "fd13865e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fd13865e",
        "outputId": "ec43125b-a71f-46dd-e6ef-f62f9f73e36f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f98009bc",
      "metadata": {
        "id": "f98009bc"
      },
      "source": [
        "## Parts:\n",
        "0. Dataset\n",
        "\n",
        "1. character-level-tokenization\n",
        "2. single self-attention module\n",
        "3. multiple attention modules (multi-head)\n",
        "4. Positional Embeddings\n",
        "5. Decoder\n",
        "6. Transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db37f934",
      "metadata": {
        "id": "db37f934"
      },
      "source": [
        "#### Part 1 : Character Level Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d81426f3",
      "metadata": {
        "id": "d81426f3"
      },
      "source": [
        "#### Part 0 : Dataset - Tiny Shakespeare"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "f9680ffc",
      "metadata": {
        "id": "f9680ffc"
      },
      "outputs": [],
      "source": [
        "class ShakespeareDataset(Dataset):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.data = pd.read_csv(\"hf://datasets/Trelis/tiny-shakespeare/train.csv\")\n",
        "        tokens = set()\n",
        "        self.maxlen = 0\n",
        "        for _, sentence in self.data.iterrows():\n",
        "            if self.maxlen < len(sentence.Text):\n",
        "                self.maxlen = len(sentence.Text)\n",
        "            for char in sentence.Text:\n",
        "                tokens.add(char)\n",
        "        self.maxlen += 2\n",
        "        self.tokens = list(tokens)\n",
        "        self.tokens.sort()\n",
        "        self.tokens.insert(0, '<pad>')\n",
        "        self.tokens.insert(1, '<eos>')\n",
        "        self.tokens.insert(2, '<sos>')\n",
        "        self.char_to_token = { c:idx for idx, c in enumerate(self.tokens) }\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data.shape[0]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        sentence = self.data.iloc[index]\n",
        "        tokenized_sentence = [self.char_to_token['<sos>']] + [self.char_to_token[char] for char in sentence.Text] + [self.char_to_token['<eos>']]\n",
        "        sample = torch.LongTensor(tokenized_sentence)\n",
        "        mask = torch.ones(self.maxlen)\n",
        "        if len(sample) < self.maxlen:\n",
        "            mask[len(sample):] = 0\n",
        "            sample = torch.cat([sample, torch.full((self.maxlen - len(sample),), self.char_to_token['<pad>'])])\n",
        "        target = torch.cat([sample[1:],torch.LongTensor([self.char_to_token['<pad>']])])\n",
        "\n",
        "        return sample, target, mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "61c48a37",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61c48a37",
        "outputId": "23be69da-8ba5-4b7c-dc30-f66973c89b5d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/divyansh/Documents/DL/NLP Transformers basics/shakespeare_transformer/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "472\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tensor([ 2, 40, 56,  ...,  0,  0,  0])"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "shsp_data = ShakespeareDataset()\n",
        "print(len(shsp_data))\n",
        "sample, target, mask =shsp_data[4]\n",
        "sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "fa1531fd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fa1531fd",
        "outputId": "2a6173fd-18db-4b65-847c-a566abc2ecfc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Your virtue is\n",
            "To make him worthy whose offence subdues him\n",
            "Your virtue is\n",
            "And curse that justice did it.\n",
            "Who deserves greatness\n",
            "Deserves your hate; and your affections are\n",
            "A sick man's appetite, who desires most that\n",
            "Which would increase his evil. He that depends\n",
            "Upon your favours swims with fins of lead\n",
            "And hews down oaks with rushes. Hang ye! Trust Ye?\n",
            "With every minute you do change a mind,\n",
            "And call him noble that was now your hate,\n",
            "Him vile that was your garland. What's the matter,\n",
            "That in these several places of the city\n",
            "You cry against the noble senate, who,\n",
            "Under the gods, keep you in awe, which else\n",
            "Would feed on one another? What's their seeking?\n",
            "\n",
            "MENENIUS:\n",
            "For corn at their own rates; whereof, they say,\n",
            "The city is well stored.\n",
            "\n",
            "MARCIUS:\n",
            "Hang 'em! They say!\n",
            "They'll sit by the fire, and presume to know\n",
            "What's done i' the Capitol; who's like to rise,\n",
            "Who thrives and who declines; side factions\n",
            "and give out\n",
            "Conjectural marriages; making parties strong\n",
            "And feebling such as stand not in their liking\n",
            "Below their cobbled shoes. They say there's\n",
            "grain enough!\n",
            "Would the nobility lay aside their ruth,\n",
            "And let me use my sword, I'll make a quarry\n",
            "With thousands of these quarter'd slaves, as high\n",
            "As I could pick my lance.\n",
            "\n",
            "MENENIUS:\n",
            "Nay, these are almost thoroughly persuaded;\n",
            "For though abundantly they lack discretion,\n",
            "Yet are they passing cowardly. But, I beseech you,\n",
            "What says the other troop?\n",
            "\n",
            "MARCIUS:\n",
            "They are dissolved: hang 'em!\n",
            "They said they were an-hungry; sigh'd forth proverbs,\n",
            "That hunger broke stone walls, that dogs must eat,\n",
            "That meat was made for mouths, that the gods sent not\n",
            "Corn for the rich men only: with these shreds\n",
            "They vented their complainings; which being answer'd,\n",
            "And a petition granted them, a strange one--\n",
            "To break the heart of generosity,\n",
            "And make bold power look pale--they threw their caps\n",
            "As they would hang them on the horns o' the moon,\n",
            "Shouting their emulation.\n",
            "\n",
            "MENENIUS:\n",
            "What is granted them?\n",
            "\n",
            "MARCIUS:\n",
            "Five tribunes to defend their vulgar wisdoms,\n",
            "Of their own choice: one's Junius Brutus,\n",
            "Sicinius Velutus, and I know not--'Sdeath!\n",
            "The rabble should have first unroof'd the city,\n",
            "Ere so prevail'd with me: it will in time\n",
            "Win upon power and throw forth greater themes\n",
            "For insurrection's arguing.\n",
            "\n",
            "MENENIUS:\n",
            "This is strange.\n",
            "\n",
            "MARCIUS:\n",
            "Go, get you home, you fragments!\n",
            "\n",
            "Messenger:\n",
            "Where's Caius Marcius?\n",
            "\n",
            "MARCIUS:\n",
            "Here: what's the matter?\n",
            "\n",
            "Messenger:\n",
            "The news is, sir, the Volsces are in arms.\n",
            "\n",
            "MARCIUS:\n",
            "I am glad on 't: then we shall ha' means to vent\n",
            "Our musty superfluity. See, our best elders.\n",
            "\n",
            "First Senator:\n",
            "Marcius, 'tis true that you have lately told us;\n",
            "The Volsces are in arms.\n",
            "\n",
            "MARCIUS:\n",
            "They have a leader,\n",
            "Tullus Aufidius, that will put you to 't.\n",
            "I sin in envying his nobility,\n",
            "And were I any thing but what I am,\n",
            "I would wish me only he.\n",
            "\n",
            "COMINIUS:\n",
            "You have fought together.\n",
            "\n",
            "MARCIUS:\n",
            "Were half to half the world by the ears and he.\n",
            "Upon my party, I'ld revolt to make\n",
            "Only my wars with him: he is a lion\n",
            "That I am proud to hunt.\n",
            "\n",
            "First Senator:\n",
            "Then, worthy Marcius,\n",
            "Attend upon Cominius to these wars.\n",
            "\n",
            "COMINIUS:\n",
            "It is your former promise.\n",
            "\n",
            "MARCIUS:\n",
            "Sir, it is;\n",
            "And I am constant.\n"
          ]
        }
      ],
      "source": [
        "sentence = ''.join([shsp_data.tokens[sample[i].item()] for i in range(len(sample)) if shsp_data.tokens[sample[i].item()] not in ['<pad>','<sos>','<eos>']])\n",
        "print(sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "4f596f9d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4f596f9d",
        "outputId": "81bc5d1f-d87b-44c2-f4bc-d2fd0910d3ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "68\n"
          ]
        }
      ],
      "source": [
        "print(len(shsp_data.tokens))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47f539b6",
      "metadata": {
        "id": "47f539b6"
      },
      "source": [
        "#### Part 2 : Single Self-Attention Head\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f03abf2a",
      "metadata": {
        "id": "f03abf2a"
      },
      "source": [
        "#### Part 3 : Multi-Head Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "0718192b",
      "metadata": {
        "id": "0718192b"
      },
      "outputs": [],
      "source": [
        "class MultiHeadedAttention(nn.Module):\n",
        "    def __init__(self, num_heads : int, model_dim : int):\n",
        "        super().__init__()\n",
        "        assert model_dim % num_heads == 0\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = model_dim // num_heads\n",
        "        self.model_dim = model_dim\n",
        "\n",
        "        self.Wq = nn.Linear(model_dim, model_dim)\n",
        "        self.Wk = nn.Linear(model_dim, model_dim)\n",
        "        self.Wv = nn.Linear(model_dim, model_dim)\n",
        "\n",
        "        self.Wo = nn.Linear(model_dim, model_dim)\n",
        "\n",
        "    def forward(self, q : torch.Tensor, k : torch.Tensor, v : torch.Tensor, mask : torch.Tensor | None = None) -> torch.Tensor:\n",
        "        # q, k, v = (bs, sl, model_dim)\n",
        "\n",
        "        bs, sl = q.shape[0], q.shape[1]\n",
        "\n",
        "        q = self.Wq(q)\n",
        "        k = self.Wk(k)\n",
        "        v = self.Wv(v)\n",
        "\n",
        "        q = q.view(bs, sl, self.num_heads, self.head_dim).transpose(1,2)\n",
        "        k = k.view(bs, sl, self.num_heads, self.head_dim).transpose(1,2)\n",
        "        v = v.view(bs, sl, self.num_heads, self.head_dim).transpose(1,2)\n",
        "\n",
        "        out = self.scaled_dot_product_attention(q,k,v,mask)\n",
        "        # out = (bs, num_heads, sl, head_dim)\n",
        "\n",
        "        out = out.transpose(1,2).contiguous().view(bs, sl, self.model_dim)\n",
        "        out = self.Wo(out)\n",
        "        return out\n",
        "\n",
        "    def scaled_dot_product_attention(self, q : torch.Tensor, k : torch.Tensor , v : torch.Tensor, mask : torch.Tensor | None =None) -> torch.Tensor:\n",
        "        out = torch.matmul(q, k.transpose(-2,-1)) / math.sqrt(self.head_dim)\n",
        "        if mask is not None:\n",
        "            if mask.shape == torch.Size([q.shape[0], q.shape[2]]):\n",
        "                #padding mask (bs, sl)\n",
        "                mask = mask.unsqueeze(1).unsqueeze(1) #(bs, 1, 1, sl)\n",
        "\n",
        "            elif mask.shape == torch.Size([q.shape[2], q.shape[2]]):\n",
        "                #causal mask (sl, sl)\n",
        "                mask = mask.unsqueeze(0).unsqueeze(0) #(1, 1, sl, sl)\n",
        "\n",
        "            elif mask.shape == torch.Size([q.shape[0],q.shape[2],q.shape[2]]):\n",
        "                #combined mask (bs, sl, sl)\n",
        "                mask = mask.unsqueeze(1) #(bs, 1, sl, sl)\n",
        "\n",
        "            else:\n",
        "                raise TypeError(f\"INPUT MASK SHAPE {mask.shape} INVALID\")\n",
        "\n",
        "            out = out.masked_fill(mask==0,value=float('-inf'))\n",
        "        out = nn.functional.softmax(out, dim=-1)\n",
        "        out = torch.matmul(out, v)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "840c72be",
      "metadata": {
        "id": "840c72be"
      },
      "source": [
        "#### Part 4 : Positional Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "376fe9df",
      "metadata": {
        "id": "376fe9df"
      },
      "outputs": [],
      "source": [
        "def positional_encoding(input : torch.Tensor) -> torch.Tensor:\n",
        "    bs, sl, model_dim = input.shape\n",
        "    pe = torch.zeros(sl, model_dim)\n",
        "\n",
        "    pos = torch.arange(0, sl).unsqueeze(1)\n",
        "    dim = torch.arange(0, model_dim, 2)\n",
        "\n",
        "    pe[:,0::2] = torch.sin(pos / torch.pow(10000,2 * dim / model_dim))\n",
        "    pe[:,1::2] = torch.cos(pos / torch.pow(10000,2 * dim / model_dim))\n",
        "\n",
        "    return input + pe.unsqueeze(0).to(input.device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "1896f1d7",
      "metadata": {
        "id": "1896f1d7"
      },
      "outputs": [],
      "source": [
        "class LayerNormalization(nn.Module):\n",
        "    def __init__(self, model_dim : int):\n",
        "        super().__init__()\n",
        "        self.gamma = nn.Parameter(torch.ones(model_dim))\n",
        "        self.beta = nn.Parameter(torch.zeros(model_dim))\n",
        "\n",
        "    def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
        "        mean = x.mean(dim=-1, keepdim=True)\n",
        "        std = x.std(dim=-1,keepdim=True)\n",
        "        return self.gamma * (x - mean) / std + self.beta"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d9ff47f",
      "metadata": {
        "id": "8d9ff47f"
      },
      "source": [
        "#### Part 5 : Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "42e031e9",
      "metadata": {
        "id": "42e031e9"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, model_dim : int, num_heads : int):\n",
        "        super().__init__()\n",
        "        self.attn1 = MultiHeadedAttention(num_heads,model_dim)\n",
        "        self.norm1 = LayerNormalization(model_dim)\n",
        "\n",
        "        self.ffn1 = nn.Sequential(\n",
        "            nn.Linear(model_dim, model_dim),\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(model_dim, model_dim)\n",
        "        )\n",
        "        self.norm2 = LayerNormalization(model_dim)\n",
        "\n",
        "    def forward(self, outputs : torch.Tensor, mask : torch.Tensor) -> torch.Tensor:\n",
        "\n",
        "        outputs = self.norm1(outputs + self.attn1(outputs, outputs, outputs, mask))\n",
        "\n",
        "        outputs = self.norm2(outputs + self.ffn1(outputs))\n",
        "\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "821594c3",
      "metadata": {
        "id": "821594c3"
      },
      "source": [
        "#### Part 7 : Transformer Assembly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "915533f8",
      "metadata": {
        "id": "915533f8"
      },
      "outputs": [],
      "source": [
        "class DecoderTransformer(nn.Module):\n",
        "    def __init__(self, model_dim : int, num_heads : int, num_decoders : int, vocab_size : int):\n",
        "        super().__init__()\n",
        "        self.out_embed = nn.Embedding(num_embeddings=vocab_size, embedding_dim=model_dim)\n",
        "        self.decoders = nn.ModuleList(\n",
        "            [Decoder(model_dim, num_heads) for _ in range(num_decoders)]\n",
        "        )\n",
        "        self.linear_out = nn.Linear(model_dim, vocab_size)\n",
        "\n",
        "    def forward(self, outputs : torch.Tensor, padding_mask : torch.Tensor | None) -> torch.Tensor:\n",
        "        bs, sl = outputs.shape[:2]\n",
        "\n",
        "        causal_mask = torch.tril(torch.ones(sl, sl, device=outputs.device))\n",
        "\n",
        "        if padding_mask is not None:\n",
        "            combined_mask = padding_mask.unsqueeze(1) * causal_mask.unsqueeze(0) #(bs, 1, sl) * (1, sl, sl) = (bs, sl, sl)\n",
        "        else:\n",
        "            combined_mask = causal_mask\n",
        "\n",
        "        outputs = self.out_embed(outputs)\n",
        "\n",
        "        outputs = positional_encoding(outputs)\n",
        "\n",
        "        for decoder in self.decoders:\n",
        "            outputs = decoder(outputs, combined_mask)\n",
        "\n",
        "        outputs = self.linear_out(outputs)\n",
        "        return outputs # (bs, sl, vocab_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f17262cb",
      "metadata": {
        "id": "f17262cb"
      },
      "source": [
        "#### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "1ca3d922",
      "metadata": {
        "id": "1ca3d922"
      },
      "outputs": [],
      "source": [
        "def train_transformer(\n",
        "        model : nn.Module,\n",
        "        train_loader : DataLoader,\n",
        "        device=device,\n",
        "        num_epochs = 10000,\n",
        "        lr = 1e-3,\n",
        "):\n",
        "    optim = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    loss_fn = nn.CrossEntropyLoss(ignore_index=train_loader.dataset.char_to_token['<pad>'])\n",
        "    vocab_size = len(train_loader.dataset.tokens)\n",
        "    model.train()\n",
        "\n",
        "    losses = []\n",
        "    pbar = tqdm(range(num_epochs))\n",
        "    for idx in pbar:\n",
        "        epoch_loss = 0\n",
        "        for samples, targets, masks in train_loader:\n",
        "            samples = samples.to(device)\n",
        "            targets = targets.to(device)\n",
        "            masks = masks.to(device)\n",
        "            optim.zero_grad()\n",
        "            loss = loss_fn(model(samples, masks).view(-1, vocab_size), targets.view(-1))\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "            epoch_loss += loss\n",
        "            pbar.set_description(f'Epoch {idx}, Loss: {loss}')\n",
        "        losses.append(epoch_loss.detach())\n",
        "    model.eval()\n",
        "    return torch.stack(losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "6e72b7e0",
      "metadata": {
        "id": "6e72b7e0"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(ShakespeareDataset(), 8, shuffle=True, num_workers=4)\n",
        "model = DecoderTransformer(32, 8, 3, len(train_loader.dataset.tokens)).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2da0f14",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "b2da0f14",
        "outputId": "f564b7aa-c133-4935-b847-d1dea6892f08"
      },
      "outputs": [],
      "source": [
        "losses = train_transformer(model,train_loader, num_epochs=200)\n",
        "plt.plot(losses.cpu())\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2WtgOD5AdOph",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "2WtgOD5AdOph",
        "outputId": "bb07059a-36cb-454d-848a-31c4f905bb1e"
      },
      "outputs": [],
      "source": [
        "# torch.save(model.state_dict(), 'shksp_decoder_transformer_32_8_3.pth')\n",
        "# from google.colab import files\n",
        "# files.download('shksp_decoder_transformer_32_8_3.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "4905ba6d",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.load_state_dict(torch.load('shksp_decoder_transformer_32_8_3.pth'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "NzaVslOvywUF",
      "metadata": {
        "id": "NzaVslOvywUF"
      },
      "outputs": [],
      "source": [
        "def generate(model : nn.Module, init_token : str = '<sos>', maxlen : int = 3000):\n",
        "  toks = torch.zeros(1,maxlen).long().to(device)\n",
        "  toks[0,0] = shsp_data.char_to_token[init_token]\n",
        "  last_tok = toks[0,0].item()\n",
        "  mask = torch.zeros(1,maxlen).long().to(device)\n",
        "  for i in range(maxlen):\n",
        "    mask[0, 0:i+1] = 1\n",
        "    probs = torch.softmax(model(toks, mask), dim=-1) #(bs, sl, vocab_Size)\n",
        "    last_tok = torch.multinomial(probs[:,-1], 1).item()\n",
        "    toks[0,i] = last_tok\n",
        "    if last_tok == shsp_data.char_to_token['<eos>']:\n",
        "      break\n",
        "  output = ''.join(\n",
        "      [shsp_data.tokens[tok] for tok in toks[0] if tok != 0]\n",
        "  )\n",
        "  return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "AZZg2f954ou7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "id": "AZZg2f954ou7",
        "outputId": "95e149e3-56f5-4882-db5e-18fadedff24f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "oa\n",
            "eeeeaeameeaeeaa:aeeaheeea eeehaereeaeaaaaarreeaaaaeree\n",
            "reehaaheweeeaehh\n",
            "areaaee\n",
            "\n",
            "reeeh\n",
            "ee!rsraeeerwh\n",
            "aeOe-reaehheehOOeeaeae\n",
            "raawrhUUeUrieaaaehUeUheaeaeeeaeeeeeeryaeUeeUaRUhwaeTeeeraeeo\n",
            "\n",
            "u\n",
            "\n",
            "-a\n",
            "\n",
            "e\n",
            "aueee\n",
            "\n",
            "i\n",
            "eee\n",
            "\n",
            "aeeaaaae\n",
            "hee\n",
            "hrr\n",
            "ehxeUa:e\n",
            "uaa\n",
            "\n",
            "eaue\n",
            "heeraeeeeaireeaaereeeRaeeeaeeeeahieeeereeeOmeeaaaeehaeetyeekrer\n",
            "ueeareeeahreeaeeuheeaaaeehayareuueaieeheaeeaereeeaaeeeeeeaee\n",
            "eaeehueeeaaaxeaeeeee\n",
            "iaeeeeyeeeaeseeeeeaeeeeaeeeekeeeal\n",
            "healm\n",
            "aeeeerueaoeia\n",
            "eeeahaaepeeeaeeaeeaaeiaaeweeeaaeaweleaeee\n",
            "eaaaew\n",
            "Raeeeeheaeaaxeye\n",
            "h\n",
            "aae\n",
            "eeieeeeea\n",
            "ee\n",
            "eeeseaeeueereeeee\n",
            "wreeaee\n",
            "eehe?aee\n",
            "eaeeehaaaeieaaeeeeataeaoeeeeaeieeahhaaeBaeeeeeeeaaea\n",
            "eeree\n",
            "eeeuoaeieaeieieeeeaiieeaeeereiaaee?aaeaeaereeh\n",
            "eleeee\n",
            "rreuh\n",
            "hhweeeueaeeeeew\n",
            "eeeuceaaeaeaeaepeereeeeteaheheeakuyyueeeeeutao\n",
            "eeeaee\n",
            "eae.e;e.euae\n",
            "\n",
            "ae;\n",
            "e\n",
            "heeaie\n",
            "ee\n",
            "ee?h-uwe\n",
            "emeeaia.\n",
            "uaaue\n",
            "Eedreeeqenehireeeaepeeueewlomexaeeewle\n",
            "ah\n",
            "aumaeaxxe.taeeeeeemaareaumR\n",
            "mea\n",
            "oaaeeeeafeewaeeeeae.eeeeyeeueaeeaeenxebmueeeeneteepaeeweaohareaaaeatieeeppeeeeee;eoaeeeeaa:eeeeera\n",
            "etaptesuaterateauaye\n",
            "aeeeaeeieeuaaeieieeaeieeeaoaeaaiaaateaeiieoreeaaeaeaawine\n",
            "eirraeaylaaieeeaeeiaeaaaieeeuoaanepeeeraaeeeraaweiaaaaree\n",
            "eeei\n",
            "alaeeeaaaeaee\n",
            "aeaeeeaaaeaoaeaapoeeaaeuuraueaaeauaeeeeaee\n",
            "aeaeeee\n",
            "aeaieeaaieepmeeeeaeyeaeiaau\n",
            "iieapeepieeeeaeeeaiparaeai\n",
            "aoee\n",
            "aeeuae\n",
            "aaaeieheaeeeeeaeaeieeueeahueaeeeueaeeaauiaaeteeeaeeuee\n",
            "thaeaaaeeyeaaieeeaeeeeeeioieeeuuaeeoeeeeeaeeieaueaaeaeeeeaaaeiee\n",
            "eoeeaee.\n",
            "aiaa\n",
            "ei\n",
            "\n",
            "eeaeeai\n",
            "\n",
            "\n",
            "aiie\n",
            "ee\n",
            "e\n",
            "ae\n",
            "\n",
            "eaiie\n",
            "e\n",
            "\n",
            "oaeaee\n",
            "aeaaitib\n",
            "Eeepeea\n",
            "aeiaeeeewaeebaneeaeeaetaeheaeepeeaeeupeeaeeaeaaeeeuaeeceammiaepaaeeaaaa\n",
            "\n",
            "aeuetaewpaeeaarueeeateeateeaaeaaeiaaiaeeetaeaaiieataateneeetmeaaaaeauneetaueatteaepeaaeainegeuaijateteeaaaleeeeeiteeameaiaeeeitauaeeamiauenttaaoeaeiame\n",
            "araaiiieetaubeptehie\n",
            "aaaeeiaaiaeeaaeiiioaeaeeeaeeatiaeyeeeiaaaaaoiaaaaaaauia\n",
            "aeahtaaeeaeaaeeeaeaiuaiaeeemeaaueyeeeaaeaaaaoaai\n",
            "oeaiaaae\n",
            "eeraeee\n",
            "eeeaeoieaaaeeeeeaeaeaueaeaeaaaeeoriiaeeeehaapaeeeaaeaeae\n",
            "eeeeaaaeeeaeueoeea\n",
            "\n",
            "\n",
            "e\n",
            "aasi\n",
            "\n",
            "aeeaa\n",
            "uei\n",
            "\n",
            "u\n",
            "ao\n",
            "e\n",
            "aa\n",
            "ae\n",
            "eaeaaee\n",
            "aaeaahebae\n",
            "eaeaaaaaeoeeaaeaeaeeaeaaiieieeaieeaeaeaoeeeauaahee\n",
            "aeeeee\n",
            "uaiaaaeeeaeaaeeeaaaeeaaaaoehaeaaaaaaeaaeahpahgeaiteaeeiuaie\n",
            "uiaaoahte\n",
            "iaee?ea\n",
            "eaeeaaeeaeeeeheaaeeetueehheeeaaiaba\n",
            "aeaeae\n",
            "eaiaib\n",
            "aetreaieeaerpeheeeaalairaeeaouaeaaaeeeeaaetepreaaaarahe\n",
            "eeeea\n",
            "aeeaaaueeaaaaaeeaeauaaaaieaeeeaaoaeaieeaiaeeraaaaeaeye\n",
            "taaaaee\n",
            "haaeaeaeeeeeaelarehageeaeteaeeaheeaeaaaahaeeeeaneaea\n",
            "heeee\n",
            "aaehaehae\n",
            "eaeiaeeeeeaaeiaaeheeaaeeeeeapeahoehaeeeeaeaa\n",
            "eeeo\n",
            "haehe:e\n",
            "eeeapeyeeeea\n",
            "aeyeaeiaaeaeoeaaaeeeauaeeaauap\n",
            "y\n",
            "eeeee\n",
            "eeeee\n",
            "eaeeeeaoa\n",
            "aaoaaee\n",
            "aaeaaleeaeaaaiuaaeueaaaeaeaeeaeee.\n",
            "ea\n",
            "eee\n",
            "eeras\n",
            "ee::epme!ei\n",
            "yempetereeeueeeae.utraeiapue\n",
            "eeemsawieaem\n",
            "aEnmemnrstmtel:eetetncsmu:tmT steeeaec::mne:eeaspmsater: mseusieetemnem:pel,'net: eeseeer.ae arueyea.hraer!usaepmpmh\n",
            "ea\n",
            "anieteje:oqeee:seeem\n",
            "evse\n",
            ",'el:ueee!b.eee:te:ele it;eeeeaeeehecebweeeeeaeeeae aea:eesa.pee!uwesccfcaeeetqemeqmaeencbaee;aeyeeaeeal qwyee\n",
            "eee\n",
            "ealexaeqaeaepeeyeyyeaeraarewaeaxbyesabeeea,eeiemeeaeaeaeaaewe\n",
            "eeeaee:leusea\n",
            "peeeermaeasaaaraoueye\n",
            "efeumaeeebeeeeb\n",
            "eeyieea\n",
            "eeaaeeure\n",
            "e\n",
            "eaweebayeeateuueeeeeubaelaebaueaaeaeir\n"
          ]
        }
      ],
      "source": [
        "output = generate(model)\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cff39a3c",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Available tokens:\", shsp_data.tokens)\n",
        "print(\"Looking for:\", '<sos>')\n",
        "print(\"Token ID:\", shsp_data.char_to_token.get('<sos>', \"NOT FOUND\"))\n",
        "print(\"Embedding expects vocab size:\", model.out_embed.num_embeddings)  # or however you access it\n",
        "print(\"Actual vocab size:\", len(shsp_data.tokens))\n",
        "print(\"Max token ID:\", max(shsp_data.char_to_token.values()))\n",
        "print(\"Min token ID:\", min(shsp_data.char_to_token.values()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a798a32",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test if your model can handle a simple input:\n",
        "test_input = torch.tensor([[2]]).to(device)  # Just <sos> token\n",
        "test_mask = torch.ones(1, 1).to(device)\n",
        "\n",
        "try:\n",
        "    test_output = model(test_input, test_mask)\n",
        "    print(\"Model forward pass successful\")\n",
        "    print(\"Output shape:\", test_output.shape)\n",
        "except RuntimeError as e:\n",
        "    print(\"Model forward pass failed:\", e)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "shakespeare_transformer",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
